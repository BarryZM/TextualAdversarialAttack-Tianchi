{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from performance_evaluator import PerformanceEvaluator\n",
    "from model import FastTextInferenceModel,BertInferenceModel,get_bert_inference_model\n",
    "from identification import *\n",
    "from manipulate import *\n",
    "from transform_workflow import ObscenityAttacker\n",
    "import jieba"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "fasttext_model_path = 'data/materials/mini.ftz'\n",
    "fasttext_model = FastTextInferenceModel(fasttext_model_path)\n",
    "kw_identify_model = fasttext_model\n",
    "\n",
    "\n",
    "attack_bert_model_path='ckpt/clf/ernie'\n",
    "attack_model=get_bert_inference_model(attack_bert_model_path, 128, 128)# 本地对抗训练得到的防御模型\n",
    "\n",
    "remote_bert_model_path = 'ckpt/clf/ernie_weibo'\n",
    "remote_defence_model = get_bert_inference_model(remote_bert_model_path, 128, 128)   # 模拟远程的模型，可以用某个早期的防御模型，用来测试对抗模型的效果\n",
    "\n",
    "tokenizer = lambda x: list(map(str, list(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "obs_attacker = ObscenityAttacker(kw_identify_model, attack_model, tokenizer)\n",
    "\n",
    "vec_emb_path = 'data/materials/zh.300.vec.gz'\n",
    "performance_evaluator = PerformanceEvaluator(vec_emb_path, defence_model=remote_defence_model)\n",
    "\n",
    "  \n",
    "kw_identification = SingleCharIdentification(kw_identify_model)  # 用本地的attack model识别关键词进行替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# ## 全局transform, 效果比较随缘(收到句子长度以及随机出来的offset的限制，而且有可能换了两个不关键的词)\n",
    "# \n",
    "# obscenities=['配你妈的字，司马玩意儿', '要点b脸', '你妈死了', '网络暴力的请您死个妈助助兴先']\n",
    "# transform = ChineseSwapTransform()\n",
    "# transformed_texts=transform(obscenities,window_size=2)\n",
    "# ref_texts = obscenities\n",
    "# print(transformed_texts)\n",
    "# print(performance_evaluator.calc_final_score(ref_texts, transformed_texts, show_details=True))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Building prefix dict from C:\\Users\\zsf\\Anaconda3\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\zsf\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.149 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "D:\\MyProjectsRepertory\\PythonProject\\Tianchi-AAT\\model\\inference.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(logits)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "['配', '你', '妈', '的', '字', '，', '司马', '玩意儿']\n",
      "[(2, 0.11794120073318481), (6, 0.08504247665405273), (1, 0.060197412967681885)]\n",
      "['配你_妈的字，司马玩意儿', '配你妈的字，_司马玩意儿', '配_你妈的字，司马玩意儿']\n",
      "Drop baseline: [0.00282179 0.00203013 0.00204636]\n",
      "[0.00224587 0.00210986 0.00228238]\n",
      "['要点', 'b', '脸']\n",
      "[(1, 0.6341887712478638), (2, 0.426460862159729), (0, -0.32009822130203247)]\n",
      "['要点_b脸', '要点b_脸', '_要点b脸']\n",
      "Drop baseline: [0.08089886 0.00556788 0.00320629]\n",
      "[0.00374125 0.00427622 0.0185371 ]\n",
      "['你', '妈', '死', '了']\n",
      "[(0, 0.952131450176239), (2, 0.17159974575042725), (1, 0.14927148818969727)]\n",
      "['_你妈死了', '你妈_死了', '你_妈死了']\n",
      "Drop baseline: [0.01424462 0.00221793 0.80746229]\n",
      "[0.03094233 0.0063145  0.017806  ]\n",
      "['网络', '暴力', '的', '请', '您', '死个', '妈助', '助兴', '先']\n",
      "[(5, 0.3227611780166626), (6, 0.028985977172851562), (2, -0.0064048171043396)]\n",
      "['网络暴力的请您_死个妈助助兴先', '网络暴力的请您死个_妈助助兴先', '网络暴力_的请您死个妈助助兴先']\n",
      "Drop baseline: [0.0230873  0.09521689 0.03799169]\n",
      "[0.01274117 0.01308929 0.02181757]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## 局部transform    \n",
    "token_swap_transform = TokenSwapTransform()\n",
    "char_swap_transform = CharSwapTransform()\n",
    "add_transform = AddTransform()\n",
    "token_drop_transform = TokenDropTransform()\n",
    "char_drop_transform = CharDropTransform()\n",
    "phonetic_transform = PhoneticTransform()\n",
    "radical_transform=RadicalTransform('data/chaizi/chaizi-jt.txt')\n",
    "pronunciation_transform=PronunciationTransform('data/chaizi/中国所有汉字大全 一行一个.txt')\n",
    "\n",
    "for text in ['配你妈的字，司马玩意儿', '要点b脸', '你妈死了', '网络暴力的请您死个妈助助兴先']:\n",
    "    tokens = tokenizer(text)\n",
    "    print(tokens)\n",
    "    key_tokens = kw_identification(tokens, N=3)\n",
    "    print(key_tokens)\n",
    "    \n",
    "    \n",
    "    drop_transformed_texts = []  # 当做替换的baseline\n",
    "    transformed_texts = []\n",
    "    for idx, score in key_tokens:\n",
    "        \n",
    "      drop_transformed_tokens = basic_transform.drop(tokens, idx)   # baseline\n",
    "      drop_transformed_texts.append(''.join(drop_transformed_tokens))\n",
    "      \n",
    "      transformed_tokens = basic_transform.add(tokens, idx)   \n",
    "      # transformed_tokens = basic_transform.swap(tokens, idx)    # word lvl的swap很垃圾，实际用的时候需要换上char lvl的\n",
    "      # transformed_tokens = phonetic_transform(tokens, idx)\n",
    "      # transformed_tokens = radical_transform.transform(tokens, idx) # 需要注意一些非左右结构的字，比如死、司等\n",
    "      # transformed_texts.append(''.join(transformed_tokens))\n",
    "      \n",
    "      # ## fixme: 下面这个是workflow中的小环节，属于特例\n",
    "      # candidates_list = pronunciation_transform(tokens, idx,N=None)\n",
    "      # transformed_tokens=tokens[:idx]\n",
    "      # for raw_char,candidates in zip(tokens[idx],candidates_list):\n",
    "      #     for candidate in candidates:\n",
    "      #         if candidate!=raw_char:\n",
    "      #             transformed_tokens.append(candidate)\n",
    "      #             break\n",
    "      # transformed_tokens+=tokens[idx+1:]\n",
    "      \n",
    "      transformed_texts.append(''.join(transformed_tokens))\n",
    "    ref_texts = [text] * len(transformed_texts)\n",
    "    print(transformed_texts)\n",
    "    print('Drop baseline:',performance_evaluator.calc_final_score(ref_texts, drop_transformed_texts, show_details=False))\n",
    "    print(performance_evaluator.calc_final_score(ref_texts, transformed_texts, show_details=False))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "  0%|          | 0/206 [00:00<?, ?it/s]D:\\MyProjectsRepertory\\PythonProject\\Tianchi-AAT\\model\\inference.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(logits)\n",
      "100%|██████████| 206/206 [03:49<00:00,  1.12s/it]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Finished\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## 遍历+log的写法\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "obscenities = []\n",
    "obscenities_set = set()\n",
    "with open('data/obscenities.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    obscenities.append(line.strip())\n",
    "    obscenities_set.add(line.strip())\n",
    "    \n",
    "## 局部transform    \n",
    "kw_identification = SingleCharIdentification(attack_model)\n",
    "\n",
    "basic_transform=BasicTransform()\n",
    "phonetic_transform = PhoneticTransform()\n",
    "radical_transform=RadicalTransform('data/chaizi/chaizi-jt.txt')\n",
    "pronunciation_transform=PronunciationTransform('data/chaizi/中国所有汉字大全 一行一个.txt')\n",
    "\n",
    "attack='fasttext'\n",
    "defence='bert'\n",
    "log_wf=open('eval_attack=%s_defence=%s.txt'%(attack,defence),'w',encoding='utf-8')\n",
    "for text in tqdm(obscenities):\n",
    "    tokens = tokenizer(text)\n",
    "    key_tokens = kw_identification(tokens, N=len(tokens))\n",
    "    \n",
    "    transformed_texts=[]\n",
    "    transformed_tokens_list=[]\n",
    "    for idx, score in key_tokens:\n",
    "      transformed_tokens_list.append(basic_transform.drop(tokens, idx))# baseline\n",
    "      transformed_tokens_list.append(basic_transform.add(tokens, idx))\n",
    "      \n",
    "      # transformed_tokens_list.append(basic_transform.swap(tokens, idx))    # word lvl的swap很垃圾，实际用的时候需要换上char lvl的\n",
    "      transformed_tokens_list.append(phonetic_transform(tokens, idx))\n",
    "      # transformed_tokens_list.append(radical_transform.transform(tokens, idx))  # 需要注意一些非左右结构的字，比如死、司等\n",
    "      \n",
    "      # ## fixme: 下面这个是workflow中的小环节，属于特例\n",
    "      candidates_list = pronunciation_transform(tokens, idx,N=None)\n",
    "      transformed_tokens=tokens[:idx]\n",
    "      for raw_char,candidates in zip(tokens[idx],candidates_list):\n",
    "          for candidate in candidates:\n",
    "              if candidate!=raw_char:\n",
    "                  transformed_tokens.append(candidate)\n",
    "                  break\n",
    "      transformed_tokens+=tokens[idx+1:]\n",
    "      transformed_tokens_list.append(transformed_tokens)\n",
    "    for transformed_tokens in  transformed_tokens_list:\n",
    "      transformed_texts.append(''.join(transformed_tokens))\n",
    "      \n",
    "    ref_texts = [text] * len(transformed_texts)\n",
    "    eval_scores=performance_evaluator.calc_final_score(ref_texts, transformed_texts, show_details=False)\n",
    "    \n",
    "    sorted_eval_scores=sorted(enumerate(eval_scores),key=lambda d:d[1],reverse=True)\n",
    "    transformed_texts=[transformed_texts[i] for i,_ in sorted_eval_scores]\n",
    "    transformed_tokens_list=[transformed_tokens_list[i] for i,_ in sorted_eval_scores]\n",
    "    eval_scores=[eval_scores[i] for i,_ in sorted_eval_scores]\n",
    "    info={\n",
    "        'ref_text':text,\n",
    "        'ref_tokens':tokens,\n",
    "        'transformed_texts':transformed_texts,\n",
    "        'transformed_tokens_list':transformed_tokens_list,\n",
    "        'eval_scores':eval_scores\n",
    "    }\n",
    "    log_wf.write('%s\\n'%(json.dumps(info,ensure_ascii=False)))\n",
    "log_wf.close()    \n",
    "print('Finished')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}